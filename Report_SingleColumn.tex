\documentclass[11pt]{article}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{booktabs}
\usepackage{url}
\usepackage[margin=1in]{geometry}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{hyperref}  % Added for hyperlinks

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=blue,
}

\lstset{
  basicstyle=\small\ttfamily,
  breaklines=true,
  breakatwhitespace=false,
  columns=flexible,
  keepspaces=true,
  showstringspaces=false,
  commentstyle=\color{gray},
  frame=single,
  framesep=3pt,
  framexleftmargin=3pt,
  xleftmargin=3pt,
  xrightmargin=3pt,
  belowskip=\smallskipamount,
  aboveskip=\smallskipamount
}

\title{\Large \textbf{Signal Denoising in Seismology Using Low-Rank Matrix Approximation Techniques}}
\author{Ronak Gadhiya (B22AI052) \\ 
        Arnava Srivastava (B22AI009) \\
        Aditya Mundhara (B22AI057) \\
        Sushrut Barmate (B22AI040) \\
        Department of Computer Science and Engineering, IIT Jodhpur \\
        Maths for Big Data}
\date{\href{https://github.com/ronakgadhiya09/SeismicDenoise}{GitHub Repository: github.com/ronakgadhiya09/SeismicDenoise}}

\begin{document}

\maketitle

\begin{abstract}
This Report presents a comprehensive mathematical analysis and implementation of low-rank matrix approximation techniques for seismic signal denoising. We investigate three principal methods: Singular Value Decomposition (SVD), Robust Principal Component Analysis (RPCA), and Hankel matrix SVD (also known as Singular Spectrum Analysis). Each method is mathematically formulated, algorithmically implemented, and experimentally evaluated using synthetic seismic data. We demonstrate that these techniques effectively separate coherent signal components from noise by exploiting the inherent low-rank structure of seismic data matrices. Performance metrics including signal-to-noise ratio and mean squared error confirm the efficacy of these approaches. Our results show that while standard SVD provides satisfactory denoising, RPCA better handles impulsive noise, and Hankel SVD effectively preserves oscillatory patterns characteristic of seismic signals.
\end{abstract}

\section{Introduction}
Seismic data processing presents significant mathematical challenges due to the complex nature of recorded signals. These signals typically contain valuable geological information obscured by various types of noise, including random background vibrations, coherent noise (e.g., surface waves), and impulsive artifacts. The extraction of clean signals from noisy measurements is thus a fundamental preprocessing step in seismological analysis.

Traditional filtering approaches often struggle to separate signal from noise when their frequency content overlaps. However, recent advances in matrix decomposition techniques have opened new avenues for signal enhancement through the mathematical concept of low-rank approximation.

This Report explores the mathematical foundations and practical implementation of three powerful low-rank approximation techniques for seismic signal denoising:

\begin{itemize}
\item Standard Singular Value Decomposition (SVD)
\item Robust Principal Component Analysis (RPCA)
\item Hankel matrix SVD (Singular Spectrum Analysis)
\end{itemize}

We provide a rigorous mathematical treatment of each method, followed by algorithmic implementation details and experimental results on synthetic seismic data sets.

\section{Mathematical Background}
\subsection{Matrix Representation of Seismic Data}

Seismic data naturally forms a matrix $\mathbf{X} \in \mathbb{R}^{m \times n}$ where each row represents a trace (recording from a single sensor) and each column represents a time sample. For multichannel recordings with $m$ traces, each containing $n$ time samples, we obtain:

\begin{equation}
\mathbf{X} = 
\begin{bmatrix}
x_{1,1} & x_{1,2} & \cdots & x_{1,n} \\
x_{2,1} & x_{2,2} & \cdots & x_{2,n} \\
\vdots & \vdots & \ddots & \vdots \\
x_{m,1} & x_{m,2} & \cdots & x_{m,n}
\end{bmatrix}
\end{equation}

This matrix can be conceptually decomposed as:
\begin{equation}
\mathbf{X} = \mathbf{S} + \mathbf{N}
\end{equation}

where $\mathbf{S}$ represents the clean signal component and $\mathbf{N}$ represents the noise. The key mathematical insight is that seismic signals often exhibit spatial and temporal coherence, resulting in a low-rank structure for $\mathbf{S}$, while noise typically spans all dimensions of the data space.

\subsection{Singular Value Decomposition}
The Singular Value Decomposition (SVD) of a matrix $\mathbf{X} \in \mathbb{R}^{m \times n}$ is given by:

\begin{equation}
\mathbf{X} = \mathbf{U} \mathbf{\Sigma} \mathbf{V}^T
\end{equation}

where:
\begin{itemize}
\item $\mathbf{U} \in \mathbb{R}^{m \times m}$ is an orthogonal matrix whose columns are the left singular vectors
\item $\mathbf{\Sigma} \in \mathbb{R}^{m \times n}$ is a diagonal matrix containing the singular values $\sigma_1 \geq \sigma_2 \geq \ldots \geq \sigma_{\min(m,n)} \geq 0$
\item $\mathbf{V} \in \mathbb{R}^{n \times n}$ is an orthogonal matrix whose columns are the right singular vectors
\end{itemize}

The Eckart-Young theorem proves that the best rank-$k$ approximation of $\mathbf{X}$ in the Frobenius norm is:

\begin{equation}
\mathbf{X}_k = \sum_{i=1}^{k} \sigma_i \mathbf{u}_i \mathbf{v}_i^T = \mathbf{U}_k \mathbf{\Sigma}_k \mathbf{V}_k^T
\end{equation}

where $\mathbf{U}_k$ contains the first $k$ columns of $\mathbf{U}$, $\mathbf{\Sigma}_k$ is the top-left $k \times k$ submatrix of $\mathbf{\Sigma}$, and $\mathbf{V}_k$ contains the first $k$ columns of $\mathbf{V}$.

The approximation error is:
\begin{equation}
\|\mathbf{X} - \mathbf{X}_k\|_F^2 = \sum_{i=k+1}^{\min(m,n)} \sigma_i^2
\end{equation}

This formulation allows us to construct a denoised approximation by retaining only the dominant singular values, which typically capture the coherent signal structure, while discarding smaller singular values associated with noise.

\subsection{Robust Principal Component Analysis}
Robust Principal Component Analysis (RPCA) extends the basic low-rank approximation by explicitly modeling the matrix as the sum of a low-rank component and a sparse component:

\begin{equation}
\mathbf{X} = \mathbf{L} + \mathbf{S}
\end{equation}

where $\mathbf{L}$ is low-rank (the coherent signal) and $\mathbf{S}$ is sparse (the noise or outliers).

The mathematical formulation is an optimization problem:

\begin{equation}
\min_{\mathbf{L}, \mathbf{S}} \|\mathbf{L}\|_* + \lambda \|\mathbf{S}\|_1 \quad \text{subject to} \quad \mathbf{X} = \mathbf{L} + \mathbf{S}
\end{equation}

where:
\begin{itemize}
\item $\|\mathbf{L}\|_* = \sum_{i} \sigma_i(\mathbf{L})$ is the nuclear norm (sum of singular values), which serves as a convex relaxation of rank
\item $\|\mathbf{S}\|_1 = \sum_{i,j} |s_{i,j}|$ is the $L_1$ norm that promotes sparsity
\item $\lambda > 0$ is a regularization parameter balancing the two components
\end{itemize}

This convex optimization problem can be solved using the Alternating Direction Method of Multipliers (ADMM), which iteratively updates $\mathbf{L}$ and $\mathbf{S}$ while enforcing the constraint.

The update rules are:

\begin{align}
\mathbf{L}_{k+1} &= \mathcal{D}_{\frac{1}{\mu}}(\mathbf{X} - \mathbf{S}_k + \frac{1}{\mu}\mathbf{Y}_k) \\
\mathbf{S}_{k+1} &= \mathcal{S}_{\frac{\lambda}{\mu}}(\mathbf{X} - \mathbf{L}_{k+1} + \frac{1}{\mu}\mathbf{Y}_k) \\
\mathbf{Y}_{k+1} &= \mathbf{Y}_k + \mu(\mathbf{X} - \mathbf{L}_{k+1} - \mathbf{S}_{k+1})
\end{align}

where:
\begin{itemize}
\item $\mathcal{D}_{\tau}$ is the singular value thresholding operator: $\mathcal{D}_{\tau}(\mathbf{A}) = \mathbf{U}\text{diag}(\max(\sigma_i - \tau, 0))\mathbf{V}^T$
\item $\mathcal{S}_{\tau}$ is the soft-thresholding operator: $\mathcal{S}_{\tau}(x) = \text{sign}(x)\max(|x| - \tau, 0)$ applied element-wise
\item $\mathbf{Y}$ is the Lagrange multiplier and $\mu > 0$ is a step size parameter
\end{itemize}

The mathematical advantage of RPCA for seismic denoising is its ability to handle impulsive noise and outliers that would otherwise corrupt standard SVD approaches.

\subsection{Hankel Matrix SVD / Singular Spectrum Analysis}
Hankel matrix SVD, also known as Singular Spectrum Analysis (SSA), operates on individual time series by embedding them into a structured matrix. For a single seismic trace $\mathbf{x} = [x_1, x_2, \ldots, x_n]$, we construct a Hankel matrix $\mathbf{H} \in \mathbb{R}^{L \times K}$ where $L$ is the window length and $K = n - L + 1$:

\begin{equation}
\mathbf{H} = 
\begin{bmatrix}
x_1 & x_2 & \cdots & x_K \\
x_2 & x_3 & \cdots & x_{K+1} \\
\vdots & \vdots & \ddots & \vdots \\
x_L & x_{L+1} & \cdots & x_n
\end{bmatrix}
\end{equation}

This matrix has constant values along each anti-diagonal. The mathematical significance of the Hankel structure lies in its connection to linear recurrent formulae that govern many natural signals, including seismic waves.

The Hankel SVD procedure follows four steps:
\begin{enumerate}
\item \textbf{Embedding}: Construct the Hankel matrix $\mathbf{H}$ as above
\item \textbf{Decomposition}: Compute the SVD of $\mathbf{H} = \mathbf{U} \mathbf{\Sigma} \mathbf{V}^T$
\item \textbf{Grouping}: Select the first $r$ components and compute $\mathbf{\hat{H}} = \sum_{i=1}^{r} \sigma_i \mathbf{u}_i \mathbf{v}_i^T$
\item \textbf{Diagonal Averaging}: Reconstruct the denoised time series by averaging along the anti-diagonals of $\mathbf{\hat{H}}$
\end{enumerate}

For the diagonal averaging step, the mathematical formula is:

\begin{equation}
\hat{x}_k = \frac{1}{N_k} \sum_{(i,j): i+j=k+1} \hat{h}_{i,j}
\end{equation}

where $N_k$ is the number of elements on the $k$-th anti-diagonal.

The window length $L$ is a critical parameter that determines the embedding dimension and should ideally capture the characteristic wavelength of the signal. The mathematical insight is that Hankel SVD performs a decomposition in a basis of data-adaptive waveforms, making it particularly suitable for oscillatory signals like seismic waves.

\section{Algorithmic Implementation}
\subsection{SVD Denoising Algorithm}

\begin{algorithm}
\caption{SVD Denoising}
\begin{algorithmic}[1]
\REQUIRE Data matrix $\mathbf{X} \in \mathbb{R}^{m \times n}$, rank $k$
\ENSURE Denoised matrix $\mathbf{\hat{X}}$
\STATE Compute SVD: $\mathbf{X} = \mathbf{U} \mathbf{\Sigma} \mathbf{V}^T$
\STATE Truncate to rank $k$: $\mathbf{U}_k, \mathbf{\Sigma}_k, \mathbf{V}_k$
\STATE Reconstruct: $\mathbf{\hat{X}} = \mathbf{U}_k \mathbf{\Sigma}_k \mathbf{V}_k^T$
\RETURN $\mathbf{\hat{X}}$
\end{algorithmic}
\end{algorithm}

The SVD approach offers computational simplicity with computational complexity of $O(\min(mn^2, m^2n))$ for an $m \times n$ matrix. The key mathematical decision is selecting the optimal rank $k$. This can be done using:

\begin{enumerate}
\item Fixed rank based on domain knowledge
\item Threshold on singular values: $\sigma_i > \tau \cdot \sigma_1$
\item Energy fraction: $\sum_{i=1}^{k} \sigma_i^2 / \sum_{i=1}^{\min(m,n)} \sigma_i^2 > \alpha$
\end{enumerate}

\subsection{RPCA Denoising Algorithm}

\begin{algorithm}
\caption{RPCA Denoising via ADMM}
\begin{algorithmic}[1]
\REQUIRE Data matrix $\mathbf{X}$, regularization $\lambda$, rank $r$, tolerance $\varepsilon$, max iterations $N$
\ENSURE Low-rank component $\mathbf{L}$, sparse component $\mathbf{S}$
\STATE Initialize: $\mathbf{L}_0 = \mathbf{0}$, $\mathbf{S}_0 = \mathbf{0}$, $\mathbf{Y}_0 = \mathbf{0}$, $\mu = 1.25/\|\mathbf{X}\|_2$, $\rho = 1.5$
\FOR{$k = 0$ to $N-1$}
\STATE Compute $\mathbf{U}\mathbf{\Sigma}\mathbf{V}^T = \text{SVD}(\mathbf{X} - \mathbf{S}_k + \mu^{-1}\mathbf{Y}_k)$
\STATE Apply SVD thresholding to get $\mathbf{\Sigma}_r$ using top $r$ values
\STATE $\mathbf{L}_{k+1} = \mathbf{U}\mathbf{\Sigma}_r\mathbf{V}^T$
\STATE $\mathbf{S}_{k+1} = \mathcal{S}_{\lambda/\mu}(\mathbf{X} - \mathbf{L}_{k+1} + \mu^{-1}\mathbf{Y}_k)$
\STATE $\mathbf{Y}_{k+1} = \mathbf{Y}_k + \mu(\mathbf{X} - \mathbf{L}_{k+1} - \mathbf{S}_{k+1})$
\STATE $\mu = \min(\rho\mu, \mu_{\max})$
\IF{$\|\mathbf{X} - \mathbf{L}_{k+1} - \mathbf{S}_{k+1}\|_F / \|\mathbf{X}\|_F < \varepsilon$}
\STATE \textbf{break}
\ENDIF
\ENDFOR
\RETURN $\mathbf{L}_{k+1}$, $\mathbf{S}_{k+1}$
\end{algorithmic}
\end{algorithm}

The RPCA implementation has computational complexity dominated by the SVD step at each iteration, with total complexity of $O(N \cdot \min(mn^2, m^2n))$ for $N$ iterations.

\subsection{Hankel SVD Algorithm}

\begin{algorithm}
\caption{Hankel SVD / SSA Denoising}
\begin{algorithmic}[1]
\REQUIRE Time series $\mathbf{x} \in \mathbb{R}^n$, window length $L$, rank $r$
\ENSURE Denoised time series $\mathbf{\hat{x}}$
\STATE Form Hankel matrix $\mathbf{H} \in \mathbb{R}^{L \times (n-L+1)}$
\STATE Compute SVD: $\mathbf{H} = \mathbf{U} \mathbf{\Sigma} \mathbf{V}^T$
\STATE Truncate to rank $r$: $\mathbf{\hat{H}} = \sum_{i=1}^{r} \sigma_i \mathbf{u}_i \mathbf{v}_i^T$
\STATE Perform diagonal averaging on $\mathbf{\hat{H}}$ to obtain $\mathbf{\hat{x}}$
\RETURN $\mathbf{\hat{x}}$
\end{algorithmic}
\end{algorithm}

For multichannel data with $m$ traces, the Hankel SVD is applied independently to each trace, with computational complexity of $O(m \cdot \min(L(n-L+1)^2, L^2(n-L+1)))$.

\section{Mathematical Analysis of Denoising Performance}
The mathematical assessment of denoising performance utilizes several metrics:

\subsection{Signal-to-Noise Ratio (SNR)}
For clean signal $\mathbf{s}$ and denoised estimate $\mathbf{\hat{s}}$:

\begin{equation}
\text{SNR} = 10 \log_{10} \left( \frac{\|\mathbf{s}\|_2^2}{\|\mathbf{s} - \mathbf{\hat{s}}\|_2^2} \right)
\end{equation}

\subsection{Mean Squared Error (MSE)}
\begin{equation}
\text{MSE} = \frac{1}{n} \|\mathbf{s} - \mathbf{\hat{s}}\|_2^2 = \frac{1}{n} \sum_{i=1}^{n} (s_i - \hat{s}_i)^2
\end{equation}

\subsection{Energy Preservation}
For the eigenvalue spectrum $\{\sigma_i\}$, the energy captured by the first $k$ components is:

\begin{equation}
E_k = \frac{\sum_{i=1}^{k} \sigma_i^2}{\sum_{i=1}^{\min(m,n)} \sigma_i^2}
\end{equation}

This measure quantifies how effectively the low-rank approximation captures the signal energy.

\section{Experimental Results and Discussion}
We applied our mathematical frameworks to synthetic seismic data generated with known signal and noise components. The synthetic data consisted of Ricker wavelets (second derivatives of Gaussians) with varying central frequencies, which effectively model seismic pulses.

% \subsection{Eigenvalue Decay Analysis}
% The mathematical structure of seismic signals is revealed in the eigenvalue spectrum obtained through SVD. 

% The rapid decay confirms the low-rank nature of seismic data and provides mathematical justification for truncating the spectrum. Quantitatively, we observed that the first 5 singular values captured over 90\% of the total energy in the clean signal.

\subsection{Performance Comparison}
We conducted comprehensive evaluations using three key metrics: Signal-to-Noise Ratio (SNR), Mean Squared Error (MSE), and Energy Preservation. Tables 1-3 present our findings across different noise levels.

\begin{table}[h]
\caption{Denoising Performance Metrics (SNR in dB)}
\centering
\begin{tabular}{lccc}
\toprule
Method & Low Noise & Medium Noise & High Noise \\
\midrule
Original Noisy & 10.5 & 4.8 & 1.2 \\
SVD (rank=5) & 17.3 & 12.1 & 7.5 \\
RPCA (rank=5) & 18.2 & 13.5 & 9.1 \\
Hankel SVD (rank=5) & 16.9 & 14.7 & 10.3 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\caption{Mean Squared Error (MSE)}
\centering
\begin{tabular}{lccc}
\toprule
Method & Low Noise & Medium Noise & High Noise \\
\midrule
Original Noisy & 0.089 & 0.331 & 0.759 \\
SVD (rank=5) & 0.019 & 0.062 & 0.178 \\
RPCA (rank=5) & 0.015 & 0.045 & 0.123 \\
Hankel SVD (rank=5) & 0.020 & 0.034 & 0.093 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\caption{Energy Preservation (percentage of original signal energy)}
\centering
\begin{tabular}{lccc}
\toprule
Method & Low Noise & Medium Noise & High Noise \\
\midrule
SVD (rank=5) & 94.2\% & 91.5\% & 87.3\% \\
RPCA (rank=5) & 95.8\% & 92.3\% & 89.6\% \\
Hankel SVD (rank=5) & 92.6\% & 93.7\% & 91.2\% \\
\bottomrule
\end{tabular}
\end{table}

The mathematical analysis of results reveals important insights:

\begin{itemize}
\item Standard SVD provides a solid baseline performance, confirming the mathematical principle that noise is distributed across all dimensions.
\item RPCA performs particularly well with impulsive noise, validating its mathematical formulation for separating sparse outliers.
\item Hankel SVD excels at preserving oscillatory patterns, especially at higher noise levels, demonstrating the value of embedding time series into a structured matrix.
\end{itemize}

\subsection{Rank Sensitivity Analysis}
The choice of rank $k$ is a critical mathematical parameter. We conducted a sensitivity analysis by varying $k$ from 1 to 20 and measuring the resulting MSE.

The mathematical relationship between rank and error typically follows a convex curve, with an optimal value that balances signal preservation (higher rank) and noise reduction (lower rank). For our synthetic seismic data, the optimal rank was approximately 5-7, corresponding to the number of significant singular values in the clean signal.

\subsection{Window Length Analysis for Hankel SVD}
For Hankel SVD, the window length $L$ is an additional mathematical parameter that affects performance. Mathematically, $L$ should relate to the periodicity of the signal. Our analysis showed:

\begin{itemize}
\item Small $L$ values ($L < 20$) failed to capture the temporal structure
\item Large $L$ values ($L > 200$) increased computational cost without significant performance gains
\item Optimal performance occurred around $L = 50-100$ for our signals with dominant periods of 20-40 samples
\end{itemize}

This confirms the mathematical relationship between the embedding dimension and the signal's characteristic wavelength.

\section{Implementation Details}
We implemented a comprehensive software framework for seismic signal denoising in Python, with a modular architecture that allows for flexible application and extension of the methods described above. The implementation consists of several key components organized in a structured codebase.

\subsection{Project Structure}
Our implementation follows a well-organized directory structure:

\begin{itemize}
\item \texttt{src/}: Source code directory
  \begin{itemize}
  \item \texttt{data/}: Data loading and handling modules
  \item \texttt{preprocessing/}: Signal preprocessing utilities
  \item \texttt{models/}: Denoising algorithm implementations
  \item \texttt{utils/}: Utility functions
  \item \texttt{visualization/}: Plotting and visualization tools
  \end{itemize}
\item \texttt{notebooks/}: Jupyter notebooks for experimentation and visualization
\item \texttt{data/}: Contains raw and processed data
\item \texttt{tests/}: Unit tests for verification
\item \texttt{docs/}: Documentation
\end{itemize}

\subsection{Tech Stack}
Our implementation leverages the following technologies and libraries:

\begin{itemize}
\item \textbf{Python 3.8+}: Core programming language
\item \textbf{NumPy}: For efficient matrix operations and numerical computing
\item \textbf{SciPy}: For scientific computing, including advanced linear algebra operations
\item \textbf{ObsPy}: For seismic data processing and importing MiniSEED files
\item \textbf{Matplotlib} and \textbf{Seaborn}: For visualization of results
\item \textbf{Pandas}: For data manipulation and analysis
\item \textbf{Scikit-learn}: For additional processing utilities
\item \textbf{Jupyter Notebooks}: For interactive experimentation and demonstration
\end{itemize}

The choice of these technologies enables high performance computation while maintaining readability and extensibility. Our framework is designed to be easily extended with additional denoising algorithms or adapted to new types of signals.

\subsection{Algorithm Implementation}
Each denoising method is implemented as a separate class following a common interface, allowing for consistent usage and comparison across methods. Key considerations in the implementation include:

\begin{itemize}
\item Memory efficiency for handling large seismic datasets
\item Vectorized operations for optimal performance
\item Robust parameter selection strategies
\item Comprehensive error handling and logging
\item Thorough documentation and type hints
\end{itemize}

The modular design allows researchers to focus on the mathematical aspects of the methods while the framework handles data loading, preprocessing, and visualization.

\section{Conclusion}
This Report has presented a comprehensive mathematical treatment of low-rank matrix approximation techniques for seismic signal denoising. We have developed the theoretical foundations, algorithmic implementations, and experimental evaluations of SVD, RPCA, and Hankel SVD approaches.

The mathematical analysis demonstrates that these methods effectively exploit the inherent low-dimensional structure of seismic signals to separate them from higher-dimensional noise. Our results confirm that while standard SVD provides good baseline performance, RPCA offers advantages for handling impulsive noise, and Hankel SVD excels at preserving the oscillatory patterns characteristic of seismic signals.

The mathematical frameworks developed in this study can be extended to other domains where signals exhibit low-rank structure in appropriate representations. Future work could explore adaptive parameter selection methods and hybrid approaches that combine the strengths of different low-rank approximation techniques.

\begin{thebibliography}{9}

\bibitem{candès2011}
Candès, E. J., Li, X., Ma, Y., \& Wright, J. (2011).
\textit{Robust Principal Component Analysis?}.
Journal of the ACM, 58(3), 1-37.

\bibitem{oropeza2011}
Oropeza, V., \& Sacchi, M. (2011).
\textit{Simultaneous Seismic Data Denoising and Reconstruction via Multichannel Singular Spectrum Analysis}.
Geophysics, 76(3), V25-V32.

\bibitem{golyandina2013}
Golyandina, N., \& Zhigljavsky, A. (2013).
\textit{Singular Spectrum Analysis for Time Series}.
Springer Science \& Business Media.

\bibitem{trickett2008}
Trickett, S. (2008).
\textit{F-xy Eigenimage Noise Suppression}.
Geophysics, 73(6), V29-V34.

\bibitem{eckart1936}
Eckart, C., \& Young, G. (1936).
\textit{The Approximation of One Matrix by Another of Lower Rank}.
Psychometrika, 1(3), 211-218.

\bibitem{boyd2011}
Boyd, S., Parikh, N., Chu, E., Peleato, B., \& Eckstein, J. (2011).
\textit{Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers}.
Foundations and Trends in Machine Learning, 3(1), 1-122.

\end{thebibliography}

\end{document} 